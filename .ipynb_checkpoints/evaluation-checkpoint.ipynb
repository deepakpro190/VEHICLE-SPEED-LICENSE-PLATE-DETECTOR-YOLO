{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a35b00-19cb-4d5d-9741-220afba7155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dell\\anaconda3\\envs\\vlpr-env\\lib\\site-packages\\ultralytics\\nn\\tasks.py:511: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(file, map_location='cpu'), file  # load\n",
      "Ultralytics YOLOv8.0.114  Python-3.8.20 torch-2.4.1+cu124 CUDA:0 (NVIDIA GeForce RTX 3050 6GB Laptop GPU, 6144MiB)\n",
      "Model summary (fused): 168 layers, 3005843 parameters, 0 gradients\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\dell\\projects\\vehicle_license\\dataset\\labels\\val.cache... 354 images, 0 backgrounds, 0 corrupt: \u001b[0m\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 23/23 [00:09\n",
      "                   all        354        354      0.979      0.901      0.979      0.832\n",
      "Speed: 0.5ms preprocess, 8.5ms inference, 0.0ms loss, 4.0ms postprocess per image\n",
      "Results saved to \u001b[1mruns\\detect\\val4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.yolo.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([0])\n",
      "box: ultralytics.yolo.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.yolo.utils.metrics.ConfusionMatrix object at 0x000001FCB9A7BF10>\n",
      "fitness: 0.8469387903950968\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.83224])\n",
      "names: {0: 'plate'}\n",
      "plot: True\n",
      "results_dict: {'metrics/precision(B)': 0.9787059090558413, 'metrics/recall(B)': 0.9011299435028248, 'metrics/mAP50(B)': 0.979222743489686, 'metrics/mAP50-95(B)': 0.8322405733845869, 'fitness': 0.8469387903950968}\n",
      "save_dir: WindowsPath('runs/detect/val4')\n",
      "speed: {'preprocess': 0.5218174497959976, 'inference': 8.477732286614886, 'loss': 0.0014614924199163578, 'postprocess': 4.011032945018703}\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "yolo_model = YOLO(\"runs/detect/trained12/weights/best.pt\")\n",
    "eval_results = yolo_model.val(data=\"data.yaml\", imgsz=640, verbose=True)  # Enable verbose output\n",
    "print(eval_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f688ce50-0c2b-4d33-9feb-225ca861e467",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     64\u001b[0m image_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprojects/vehicle_license/dataset/images/val/2430703d-0fb3-4eb2-9765-4f9301f232cd___3e7fd381-0ae5-4421-8a70-279ee0ec1c61_Terrano-Duster-Rear-Comparison - Copy.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Change this to your test image\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(results)\n",
      "Cell \u001b[1;32mIn[21], line 11\u001b[0m, in \u001b[0;36mrun_inference\u001b[1;34m(image_path, model_path, conf)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Read the image and resize it to 640x640 for inference\u001b[39;00m\n\u001b[0;32m     10\u001b[0m img \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_path)\n\u001b[1;32m---> 11\u001b[0m original_h, original_w, _ \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m  \u001b[38;5;66;03m# Get original image dimensions\u001b[39;00m\n\u001b[0;32m     12\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(img, (\u001b[38;5;241m640\u001b[39m, \u001b[38;5;241m640\u001b[39m))  \u001b[38;5;66;03m# Resize to 640x640\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Perform inference on the resized image\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from ultralytics import YOLO\n",
    "\n",
    "def run_inference(image_path, model_path=\"runs/detect/trained12/weights/best.pt\", conf=0.5):\n",
    "    # Load the model\n",
    "    model = YOLO(model_path)\n",
    "    \n",
    "    # Read the image and resize it to 640x640 for inference\n",
    "    img = cv2.imread(image_path)\n",
    "    original_h, original_w, _ = img.shape  # Get original image dimensions\n",
    "    img_resized = cv2.resize(img, (640, 640))  # Resize to 640x640\n",
    "    \n",
    "    # Perform inference on the resized image\n",
    "    results = model(img_resized, imgsz=640, conf=conf)\n",
    "    \n",
    "    # Check if any detections were made\n",
    "    if len(results[0].boxes) == 0:\n",
    "        print(\"No objects detected in the image.\")\n",
    "        return results\n",
    "    \n",
    "    # Extract the results (boxes, labels, and confidences)\n",
    "    boxes = results[0].boxes.xywh  # [x_center, y_center, width, height]\n",
    "    confidences = results[0].boxes.conf  # Confidence scores for each detection\n",
    "    labels = results[0].names  # Labels of the detected objects\n",
    "    \n",
    "    # Print out detected objects for debugging\n",
    "    print(f\"Detected {len(boxes)} objects:\")\n",
    "    for i, box in enumerate(boxes):\n",
    "        label = labels[int(results[0].boxes.cls[i])]\n",
    "        print(f\"Object {i + 1}: {label} with confidence {confidences[i]:.2f}\")\n",
    "    \n",
    "    # Convert the resized image back to RGB for displaying with matplotlib\n",
    "    img_resized_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    # Draw bounding boxes on the resized image\n",
    "    for i, box in enumerate(boxes):\n",
    "        x_center, y_center, width, height = box\n",
    "        # Rescale the bounding boxes from 640x640 back to the original image size\n",
    "        x1 = int((x_center - width / 2) * original_w / 640)\n",
    "        y1 = int((y_center - height / 2) * original_h / 640)\n",
    "        x2 = int((x_center + width / 2) * original_w / 640)\n",
    "        y2 = int((y_center + height / 2) * original_h / 640)\n",
    "        \n",
    "        # Filter by confidence if needed (optional)\n",
    "        if confidences[i] < conf:\n",
    "            continue  # Skip boxes with low confidence\n",
    "        \n",
    "        # Draw rectangle on the resized image (showing bounding boxes in the original image scale)\n",
    "        cv2.rectangle(img_resized_rgb, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "        \n",
    "        # Display the label and confidence\n",
    "        label = f\"{labels[int(results[0].boxes.cls[i])]} {confidences[i]:.2f}\"\n",
    "        cv2.putText(img_resized_rgb, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)\n",
    "    \n",
    "    # Display the image with bounding boxes\n",
    "    plt.imshow(img_resized_rgb)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example usage:\n",
    "image_path = \"dataset/images/val/2430703d-0fb3-4eb2-9765-4f9301f232cd___3e7fd381-0ae5-4421-8a70-279ee0ec1c61_Terrano-Duster-Rear-Comparison - Copy.jpg\"  # Change this to your test image\n",
    "results = run_inference(image_path)\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36917a5-9a24-4ae7-85a6-f176e6fb0fbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd17fb2d-4cc2-4bbf-b758-e6bef3730c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
